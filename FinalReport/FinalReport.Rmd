---
title: "BIOS 735 Final Project"
author: "Madhuri Raman, Will Tang, Xuejun Sun, Sophie Shan, Jiawen Du"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{amsthm}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amscd}
  - \usepackage{amssymb}
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: yes
    keep_tex: yes 
subtitle: Group 1
---

# Introduction

In the digital age, phishing is a common practice in which cybercriminals attempt to steal sensitive, personal information from users, such as their usernames/passwords, social-security number, credit card information, etc. Phishing can occur in many different forms, but one common way is through websites. You have probably received suspicious emails before from unknown senders. These emails usually contain phrases to bait your interest and may contain a link to a website that sounds interesting, but really is just collecting your personal information. We want to avoid clicking on phishing links, while also not missing out on important non-phishing links that we need to access. 

## Motivation

The goal of this project is to be able to predict whether or not a website link is a phishing website or not, based on only features of the URL. If we ever receive an email with a link from an unknown sender, but we aren't sure if it is suspicious, we hope that this data analysis can: 
1. Predict whether the URL is phishing or not, and
2. Provide insight into what features of the URL are most important to look out for when judging whether the link is phishing or not.


## Data

In order to achieve these goals, we work with a data set from Kaggle (linked [here](https://www.kaggle.com/datasets/danielfernandon/web-page-phishing-dataset)) which contains 100,007 website URLs, 19 features of these URLs, and an indicator (0 or 1) of whether the URL is a phishing website or not. The 19 features are characteristics of each URL and includes: 

* url length
* number of dots in the URL
* number of hyphens in the URL
* number of underlines in the URL
* number of "+" characters in the URL
* number of redirections (number of times the URL redirects you to another website)

etc. Below we conduct exploratory data analysis to examine the univariate and bivariate distributions of the features and response variable (indicator of phishing).

### EDA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Sparsity

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Missingness?

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi iyean blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Visualize distributions, correlations, outliers

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

##### Histograms, corrplots, PCA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.


#### Preview of Final Data

```{r}
library()
library(knitr)
library(kableExtra)
library(data.table)
library(dplyr)
```

## Data Splitting and Setup

\noindent We first split out dataset into a training (80\%) and test set (20\%). Suppose our training set has $n$ datapoints. Then: 

\noindent Let $Y$ be the $n \times 1$ response vector:
$$ Y_i = 
\begin{cases} 
1 & \text{if webpage is phishing} \\
0 & \text{if not} 
\end{cases} \quad i = 1, \ldots, n$$. 

\noindent Let $X^T$ be the $n \times (p + 1)$ design matrix: 
\[ X^T = 
\begin{bmatrix}
1 & x_{11} & \cdots & x_{1p} \\
1 & x_{21} & \cdots & x_{2p} \\
\vdots & \ddots & \vdots \\
1 & x_{n1} & \cdots & x_{np} \\
\end{bmatrix}
\]

We use the above framework for the following data analysis, both for the likelihood-based method and machine learning methods.

# Packaging

Insert info about package and functions

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.


# Methods
## Likelihood Approach
### Generalized Linear Model

\noindent  Let $p_i = P(Y_i=1|\mathbf{x_i^T)}$. Since our response variable is binary, we fit a logistic  model on the dataset. We use the logit link. 

$$logit(p_i) = log(\frac{p_i}{1-p_i}) =  \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} = \mathbf{x_i^T} \beta$$

\noindent Since this is binary data, the individual data come from a Bernoulli distribution. Our joint likelihood function is: 

$$L(\beta|Y, n) = \prod_{i=1}^n p_i^{I(y_i=1)} (1-p_i)^{I(y_i=0)}$$

\noindent The log-likelihood is:

\begin{align}
l(\beta|Y, n) &= \sum_{i=1}^n I(y_i=1) log(p_i) + I(y_i=0) log(1-p_i) \\
&= \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)
\end{align}

### Logistic Regression with Mini-Batch SGD

We chose to use mini-batch stochastic gradient descent to find the solution for \( \beta \). This decision was based on the principle that maximizing the log-likelihood function is analogous to minimizing its negative counterpart. Therefore, we define our cost function as \( J(\beta) = -l(\beta \mid Y, n) \). The algorithm proceeds as follows:

1. Initialize the estimate for $\beta$.
2. At each iteration, select a mini-batch of size $\alpha$ from $X^T$, denoted $\{\mathbf{x_i^T}\}$, where $i$ indexes the randomly selected rows.
3. Calculate the gradient of the negative log-likelihood, $\nabla J(\beta)$, using the mini-batch sample.
4. Update $\beta$ according to the rule: $\beta_{\text{new}} = \beta_{\text{old}} - \eta \nabla J(\beta)$, where $\eta$ is the learning rate.
5. Repeat steps 2-4 until convergence, defined as $|\beta_{\text{new}} - \beta_{\text{old}}| < \epsilon$, where $\epsilon$ is a pre-specified threshold.


\noindent The gradient of the log-likelihood for our model is detailed below (for full derivation, please see the $\textbf{Appendix B}$):

$$\nabla J(\beta)^T = [ \frac{\partial J(\beta)}{\partial \beta_0} \ldots \frac{\partial J(\beta)}{\partial \beta_p} ] = \mathbf{X^T}(\mathbf{p}-\mathbf{y})$$

\begin{align}
\frac{\partial J(\beta)}{\partial \beta_j} &= \frac{\partial J(\beta)}{\partial p_i} \cdot \frac{\partial p_i}{\partial \beta_j} \\
&= \sum_{i=1}^n - \left(\frac{y_i}{p_i} + \frac{1-y_i}{1-p_i}\right) \cdot p_i(1-p_i) x_{ij} \\
&= \sum_{i=1}^n (-y_i(1-p_i) + (1-y_i)p_i) x_{ij} \\
&= \sum_{i=1}^n (p_i - y_i) x_{ij} 
\end{align}


\noindent We initialized \(\beta\) as the zero vector at the start of the algorithm and continued iterations until convergence. Convergence was defined by the condition \( \left| \beta_{\text{new}} - \beta_{\text{old}} \right| < \epsilon \), where \(\epsilon = 10^{-4}\).

\noindent First, we verified that our SGD algorithm functioned correctly through simulation. We generated 100,000 observations with 20 covariates, each drawn from a normal distribution:

$$X_i \sim \mathcal{N}(0, 1) \text{ for } i = 1, \ldots, 20$$

Given that our dataset contains only non-negative covariate values, we transformed all covariates by taking their absolute value:

$$X_{i, \text{new}} = \left| X_i \right|$$

We then assigned the coefficients a value of 0.5 or -0.5, and the outcomes were calculated using a logistic regression model with a logit link function:

$$\text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} = \mathbf{x_i}^T \beta$$

where $\beta_0 = -0.5$, $\beta_j = 0.5$ for $j = 1, \ldots, 10$, and $\beta_j = -0.5$ for $j = 11, \ldots, 20$.

The mini-batch size ($\alpha$) was set to 128, and the learning rate ($\eta$) was set to 0.001. The algorithm converged after 962 iterations, and we observed that the mean square error (MSE) between the coefficient estimates produced by our algorithm and those estimated by the GLM was only $8 \times 10^{-6}$, indicating that our algorithm performs comparably to the GLM.

```{r, eval=FALSE}
# Conduct simulation
set.seed(123)
n=1e5
X.sim = abs(matrix(rnorm(n * 20), nrow = n, ncol = 20))
data.sim = as.data.frame(X.sim)
names(data.sim) = paste("X", 1:20, sep="")
beta.sim = c(-0.5,rep(0.5,10),rep(-0.5,10))
X.intercept.sim = cbind(1,X.sim)
prob = exp(X.intercept.sim %*% beta.sim) / (1+ exp(X.intercept.sim %*% beta.sim))
data.sim$y.sim = rbinom(n,1,prob)
mean(data.sim$y.sim)
fit.sim = glm(data = data.sim, y.sim ~.,family = "binomial")
coef(fit.sim)
lg.sim = LogisticRegression(data.sim$y.sim, X.sim, epsilon = 1e-4, batch_size = 128, learning_rate=1e-3, max_iter = 50000)
cat("MSE with true beta: ",mean((lg.sim$coefficients - beta.sim)^2))
cat("MSE with GLM estimates: ",mean((lg.sim$coefficients - coef(fit.sim))^2))
```

\noindent Next, we applied the SGD algorithm to the phishing dataset, carefully tuning the mini-batch size ($\eta$) and learning rate ($\alpha$) to optimize performance. We removed the covariates that had more than 99\% missing data: $\textit{n_exclamation}, \textit{n_space}, \textit{n_tilde}, \textit{n_comma}, \textit{n_plus}, \textit{n_asterisk}, \textit{n_hashtag}, \textit{n_dollar}$. These variables were so sparse that we believed including them in the model would not be beneficial. We selected the optimal hyperparameters ($\alpha, \eta$) by evaluating both the convergence speed and the MSE on the training set. MSE was computed as the squared differences between the parameter estimates derived from our algorithm and those generated by the General Linear Model (GLM) function from the \textbf{stats} package in R. We chose a mini-batch size ($\alpha$) of 128 and a learning rate ($\eta$) of 0.01 for our final model. 
```{r, eval=FALSE}
### tune parameters, no need to run when compiling report because of time-consuming ###

# read training and testing set
y.train = fread("derived_data/train_y.csv") %>% unlist() %>% as.vector()
X.train = fread("derived_data/train_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()
y.test = fread("derived_data/test_y.csv") %>% unlist() %>% as.vector()
X.test = fread("derived_data/test_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()

# fit GLM
fit = glm(y.train ~ X.train, family="binomial")
coef = coef(fit)

# tune our hyperparameters
set.seed(123)

tune = as.data.frame(matrix(c(rep(NA,12*4)),ncol=4))
colnames(tune) = c("batch_size","learning_rate","iteration","MSE")

i=1
set.seed(123) # for shuffle in mini batch
for (b in c(32,64,128,256)){
    for (lr in c(1e-2, 1e-3, 1e-4)){
    tune$batch_size[i] = b
    tune$learning_rate[i] = lr
    lgres = SGD_logistic(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
    tune$iteration[i] <- lgres$iter
    tune$MSE[i] <- mean((coef(fit) - lgres$coefficients)^2)
    i=i+1
    }
}
```

```{r}
# Create a data frame for tuning results
data <- data.frame(
  `batch size` = c(32, 32, 32, 64, 64, 64, 128, 128, 128, 256, 256, 256),
  `learning rate` = c(0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001),
  `iteration` = c(14582, 9643, 11364, 15856, 10476, 10829, 9864, 10795, 10415, 9513, 11118, 11120),
  `MSE` = c(3.05294096, 0.19290544, 1.12656500, 0.64438685, 0.36867774, 1.54867577, 0.02094163, 0.65899539, 1.91463586, 0.12832514, 1.00933585, 2.16481173)
)

# Create the table
comparison <- kable(data, format = "html", table.attr = "style='width:100%;'") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# Print
comparison
```

\noindent We obtained the estimated coefficients from the training set and used them to predict the binary outcome on the test set. The predictive accuracy of our gradient descent algorithm is approximately 0.851.

```{r,eval=FALSE}
# read training and testing set
y.train = fread("derived_data/train_y.csv") %>% unlist() %>% as.vector()
X.train = fread("derived_data/train_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()
y.test = fread("derived_data/test_y.csv") %>% unlist() %>% as.vector()
X.test = fread("derived_data/test_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()

# refit the best model on training
b=128
lr=0.01
lgres.train = SGD_logistic(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
coef.train = lgres.train$coefficients

# predict outcomes on testing
X.intercept.test = cbind(1,X.test)
prob = exp(X.intercept.test %*% coef.train) / (1+ exp(X.intercept.test %*% coef.train))
y.pred = ifelse(prob >= 0.5,1,0)
accuracy = sum(1*(y.test == y.pred)) / length(y.test)
cat("Accuracy:",acurracy)

library(caret)
conf_matrix = confusionMatrix(as.factor(y.pred), as.factor(y.test))
f1_score = conf_matrix$byClass['F1']
cat("F1 score:",f1_score)
```

## Machine Learning approaches

### Random Forest

### Support Vector Machines
```{r}



```

# Final predictions on test set (to "choose" best model)

```{r}



```

# Conclusion

## General thoughts

## Limitations of this project

## Future work

## Citations

GLM: 

1. https://web.stanford.edu/~jurafsky/slp3/5.pdf 

2. https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm 

3. https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/ 

4. https://rpubs.com/enwuliu/1011024 

# Apendices

## Appendix A

\noindent Expressing the logit function \( logit(p_i) \) in terms of \( p_i \):
\[logit(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \mathbf{x_i^T}\beta\]

\noindent This equation can be rewritten to solve for \( p_i \):
\[\frac{p_i}{1-p_i} = \exp(\mathbf{x_i^T}\beta)\]

\noindent Multiplying both sides by \( 1-p_i \) gives:
\[p_i = (1-p_i) \exp(\mathbf{x_i^T}\beta)\]

\noindent Expanding and rearranging terms to isolate \( p_i \) on one side:
\begin{align*}
p_i &= \exp(\mathbf{x_i^T}\beta) - p_i \exp(\mathbf{x_i^T}\beta) \\
p_i + p_i \exp(\mathbf{x_i^T}\beta) &= \exp(\mathbf{x_i^T}\beta) \\
p_i(1 + \exp(\mathbf{x_i^T}\beta)) &= \exp(\mathbf{x_i^T}\beta)
\end{align*}

\noindent Solving for \( p_i \):
\[p_i = \frac{\exp(\mathbf{x_i^T}\beta)}{1 + \exp(\mathbf{x_i^T}\beta)}\]

\noindent By factoring out \( \exp(\mathbf{x_i^T}\beta) \) in the denominator, the expression simplifies further:
\[p_i = \frac{1}{1 + \exp(-\mathbf{x_i^T}\beta)}\]

## Appendix B

\noindent Here are the calculations for the partial derivatives required for the gradient function:

\begin{align}
\frac{\partial J(\beta)}{\partial p_i} 
&= \frac{\partial}{\partial p_i} - \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)\\
&= \sum_{i=1}^n - \frac{y_i}{p_i} + \frac{(1-y_i)}{1-p_i}
\end{align}

\begin{align}
\frac{\partial p_i}{\partial \beta_j} &= \frac{\partial p_i}{\partial \beta_j}  (1 + \exp(-\mathbf{x_i^T} \beta))^{-1} \\
&= exp(-\mathbf{x_i^T} \beta) (1+ exp(-\mathbf{x_i^T}\beta))^{-2} x_{ij} \\ 
&=  \frac{(1+exp(-\mathbf{x_i^T}\beta))-1}{(1+ exp(-\mathbf{x_i^T}\beta)) (1+ exp(-\mathbf{x_i^T}\beta))} x_{ij} \\ 
&=  \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))} (1 - \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))}) x_{ij} \\
&=  p_i (1-p_i) x_{ij}
\end{align}

