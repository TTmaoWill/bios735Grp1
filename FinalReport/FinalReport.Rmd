---
title: "BIOS 735 Final Project"
author: "Madhuri Raman, Will Tang, Xuejun Sun, Sophie Shan, Jiawen Du"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: yes
    keep_tex: yes 
subtitle: Group 1
---

# Introduction

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

## Motivation

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

## Background info /description of data set

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

## EDA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

### What data it contains, size, etc

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

### Missingness?

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

### Visualize distributions, correlations, outliers

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Histograms, corrplots, PCA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

### Discuss Sparsity of features

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.


```{r}
# TODO:
# Madhuri: - Still need to set up report framework
#          - not sure if we should knit to html or pdf

```

\noindent We first split out dataset into a training (80\%) and test set (20\%). Suppose our training set has $n$ datapoints. Then: 

\noindent Let $Y$ be the response vector:
$$ Y_i = 
\begin{cases} 
1 & \text{if webpage is phishing} \\
0 & \text{if not} 
\end{cases} \quad i = 1, \ldots, n$$. 

\noindent Let $X^T$ be the $n$ by $p$ design matrix: 
\[ X^T = 
\begin{bmatrix}
x_{11} & \cdots & x_{1p} \\
x_{21} & \cdots & x_{2p} \\
\vdots & \ddots & \vdots \\
x_{n1} & \cdots & x_{np} \\
\end{bmatrix}
\]


# Generalized Linear Model

\noindent  Let $p = P(Y=1|X)$. Since our response variable is binary, we fit a logistic  model on the dataset. We use the logit link. 

$$logit(p) = log(\frac{p}{1-p}) =  X^T \beta$$

\noindent Since this is binary data, the individual data come from a Bernoulli distribution. Our joint likelihood function is: 

$$L(\beta|Y, n) = \prod_{i=1}^n p^{I(y_i=1)} (1-p)^{I(y_i=0)}$$

\noindent The log-likelihood is:

\begin{align}
l(\beta|Y, n) &= \sum_{i=1}^n I(y_i=1) log(p) + I(y_i=0) log(1-p) \\
&= \sum_{i=1}^n y_i log(p) + (1-y_i) log(1-p)
\end{align}

# Logistic Regression with Mini-Batch SGD

We chose to use mini-batch stochastic gradient descent to find the solution for \( \beta \). This decision was based on the principle that maximizing the log-likelihood function is analogous to minimizing its negative counterpart. Therefore, we define our cost function as \( J(\beta) = -l(\beta \mid Y, n) \). The algorithm proceeds as follows:

1. Initialize the estimate for $\beta$.
2. At each iteration, select a mini-batch of size $\alpha$ from $X^T$, denoted $\{x_{i.}\}$, where $i$ indexes the randomly selected rows.
3. Calculate the gradient of the log-likelihood, $\nabla J(\beta)$, using the mini-batch sample.
4. Update $\beta$ according to the rule: $\beta_{\text{new}} = \beta_{\text{old}} - \eta \nabla J(\beta)$, where $\eta$ is the learning rate.
5. Repeat steps 2-4 until convergence, defined as $|\beta_{\text{new}} - \beta_{\text{old}}| < \epsilon$, where $\epsilon$ is a pre-specified threshold.


\noindent The gradient of the log-likelihood for our model is detailed below (for full derivation, please see the \textbf{Appendix B}):

$$\nabla J(\beta)^T = [ \frac{\partial J(\beta)}{\partial \beta_1} \ldots \frac{\partial J(\beta)}{\partial \beta_p} ] $$

\begin{align}
\frac{\partial J(\beta)}{\partial \beta_j} &= \frac{\partial J(\beta)}{\partial p} \cdot \frac{\partial p}{\partial \beta_j} \\
&= \sum_{i=1}^n - \left(\frac{y_i}{p} + \frac{1-y_i}{1-p}\right) \cdot p(1-p) x_j \\
&= \sum_{i=1}^n (-y_i(1-p) + (1-y_i)p) x_j \\
&= \sum_{i=1}^n (p - y_i) x_j
\end{align}


\noindent We initialized \(\beta\) as the zero vector at the start of the algorithm and continued iterations until convergence. Convergence was defined by the condition \( \left| \beta_{\text{new}} - \beta_{\text{old}} \right| < \epsilon \), where \(\epsilon = 0.001\).

\noindent We optimized both the mini-batch size ($\alpha$) and the learning rate ($\eta$), selecting the best hyperparameters based on convergence speed and Mean Squared Error (MSE). Here, MSE was calculated as the squared differences between the parameter estimates obtained by our method and those provided by the General Linear Model (GLM) function in the \textbf{stats} package in R. 


```{r}



```

# Machine Learning approaches
```{r}



```

# Final predictions on test set (to "choose" best model)

```{r}



```

# Conclusion

## General thoughts

## Limitations of this project

## Future work

## Appendix A

\noindent Expressing the logit function \( logit(p) \) in terms of \( p \):
\[logit(p) = \log\left(\frac{p}{1-p}\right) = X^T\beta\]

\noindent This equation can be rewritten to solve for \( p \):
\[\frac{p}{1-p} = \exp(X^T\beta)\]

\noindent Multiplying both sides by \( 1-p \) gives:
\[p = (1-p) \exp(X^T \beta)\]

\noindent Expanding and rearranging terms to isolate \( p \) on one side:
\begin{align*}
p &= \exp(X^T \beta) - p \exp(X^T \beta) \\
p + p \exp(X^T \beta) &= \exp(X^T \beta) \\
p(1 + \exp(X^T \beta)) &= \exp(X^T \beta)
\end{align*}


\noindent Solving for \( p \):
\[p = \frac{\exp(X^T \beta)}{1 + \exp(X^T \beta)}\]

\noindent By factoring out \( \exp(X^T \beta) \) in the denominator, the expression simplifies further:
\[p = \frac{1}{1 + \exp(-X^T \beta)}\]

## Appendix B

\noindent Here are the calculations for the partial derivatives required for the gradient function:

\begin{align}
\frac{\partial J(\beta)}{\partial p} 
&= \frac{\partial}{\partial p} - \sum_{i=1}^n y_i log(p) + (1-y_i) log(1-p)\\
&= \sum_{i=1}^n - \frac{y_i}{p} + \frac{(1-y_i)}{1-p}
\end{align}

\begin{align}
\frac{\partial p}{\partial \beta_j} &= \frac{\partial p}{\partial \beta_j}  (1 + \exp(-X^T \beta))^{-1} \\
&= exp(-X^T \beta) (1+ exp(-X^T \beta))^{-2} x_j \\ 
&=  \frac{(1+exp(-X^T \beta))-1}{(1+ exp(-X^T \beta)) (1+ exp(-X^T \beta))} x_j \\ 
&=  \frac{1}{(1+ exp(-X^T \beta))} (1 - \frac{1}{(1+ exp(-X^T \beta))})x_j \\
&=  p (1-p)x_j
\end{align}

