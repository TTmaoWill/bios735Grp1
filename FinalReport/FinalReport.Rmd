---
title: "BIOS 735 Final Project"
author: "Madhuri Raman, Will Tang, Xuejun Sun, Sophie Shan, Jiawen Du"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{amsthm}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amscd}
  - \usepackage{amssymb}
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: yes
    keep_tex: yes
    code_folding: hide 
subtitle: Group 1
---

To reproduce the report, you need to install package [phishing735](https://github.com/madzraman/phishing735/) first. See instructions to do so in the README section of the Gihub repository.

```{r,message=F,warning=F}
library(phishing735)
```

# Introduction

In the digital age, phishing is a common practice in which cybercriminals attempt to steal sensitive, personal information from users, such as their usernames/passwords, social-security number, credit card information, etc. Phishing can occur in many different forms, but one common way is through websites. You have probably received suspicious emails before from unknown senders. These emails usually contain phrases to bait your interest and may contain a link to a website that sounds interesting, but really is just collecting your personal information. We want to avoid clicking on phishing links, while also not missing out on important non-phishing links that we need to access. 

## Motivation

The goal of this project is to be able to predict whether or not a website link is a phishing website or not, based on only features of the URL. If we ever receive an email with a link from an unknown sender, but we aren't sure if it is suspicious, we hope that this data analysis can: 
1. Predict whether the URL is phishing or not, and
2. Provide insight into what features of the URL are most important to look out for when judging whether the link is phishing or not.


## Data

In order to achieve these goals, we work with a data set from Kaggle (linked [here](https://www.kaggle.com/datasets/danielfernandon/web-page-phishing-dataset)) which contains 100,007 website URLs, 19 features of these URLs, and an indicator (0 or 1) of whether the URL is a phishing website or not. The 19 features are count characteristics of each URL, including: 

* url length
* number of dots in the URL
* number of hyphens in the URL
* number of underlines in the URL
* number of "+" characters in the URL
* number of redirections (number of times the URL redirects you to another website)

Below we conduct exploratory data analysis to examine the univariate and bivariate distributions of the features and response variable (indicator of phishing).

## Exploratory Data Analysis

```{r, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(data.table)
library(tidyverse)
library(dplyr)
```


### Data Cleanliness

There are no missing values in the original data set. The data is clean and complete as sourced and required no pre-processing. Additionally, since nearly the features are all counts (with extreme sparsity), no standardization of the covariates was performed before data splitting or modeling.

### Sparsity

Because most of the features in this data are count variables, we are curious about how much sparsity (zero counts) are in the data. In Figure 1 below, we see the percentage of each column that has zero values. There is extreme sparsity in many of the covariates and there are eight covariates highlighted in yellow for which there is over 99% sparsity, or over 99% of zero values. Although we have 100,000 rows of observations, this sparsity heavily restricts the amount of actual data we have to work with for future model fitting. Due to this sparsity, it was unnecessary to further look at individual histograms or distributions of each variable, as they would be very skewed and unreadable. In addition, this sparsity was so extreme that log-transformations or standardization of covariates would not be helpful or sensible for model fitting. The lack of data mentioned earlier is the reason why we choose to model the response against all covariates whenever possible, before considering variable importance. 

```{r, echo=FALSE, fig.cap="Figure 1.", fig.align="center"}
knitr::include_graphics("figures/sparsity_table.png")
```

We also see that the response variable `phishing` does not have extreme sparsity. In fact, the roughly 70/30 split of class labels does not even warrant adjusting for class imbalance in the model. 

### PCA, Outliers, and Correlation

We conduct PCA on all the features to get an introductory feel for the most important dimensions of the data. We plot PC1 vs. PC2 in Figure 2 below and color each point based on the true response labels indicating phishing or not. We see that there is some good separation between the classes in the bottom right section of the plot, which shows there is definitely a potential decision boundary that we can find with our future models. 

```{r, echo=FALSE, fig.cap="Figure 2.", fig.align="center"}
knitr::include_graphics("figures/pcaplot.png")
```

There is also a notable outlier circled at the top left of the plot in Figure 2. When we sort the data by decreasing URL length in Figure 3, we notice that there is an extreme value of URL length (4165) that is much larger than every other URL length. Perhaps this observation is the outlier in the PCA plot. However, it is definitely possible that a large URL length implies large counts of other character covariates, by nature, so it is possible that another column is actually motivating the outlier. 


```{r, echo=FALSE, fig.cap="Figure 3.", fig.align="center"}
knitr::include_graphics("figures/by_url_length.png")
```

We investigate this natural correlation between columns in this data by constructing a correlation plot, given in Figure 4 below. Here, we see that URL length is actually not strongly correlated with any other variables in the data, contrary to our previous expectation, so that outlier in the PCA plot is likely the extreme URL length observation. Other than one strong positive correlation of 0.9 between number of &s and number of =s, there is no other strong evidence of collinearity between covariates. Also, there are no strong correlations between the response `phishing` and any covariates; there is a correlation of 0.6 with number of /s, but otherwise nothing too notable or impactful for future model fitting.

```{r, echo=FALSE, fig.cap="Figure 4.", fig.align="center"}
knitr::include_graphics("figures/corrplot.png")
```


## Data Splitting and Setup

We split our full data set without any pre-processing into a training set with 80\% of data and test set with 20\% of data. Note that the test set was only used to compare the performance of all tuned models at the end. 

# Packaging


All code and data for this project can be found in our R package `phishing735`. This package contains the following functions. Please refer to associated `roxygen` documentation for more details about each function.

* `phishing_data_checks`
    * Inputs: none
    * Outputs: printed messages about intro data checks
* `phishing_corrs_plot`
    * Inputs: none
    * Outputs: ggplot object 
* `split_phishing_data`
    * Inputs: none
    * Outputs: csv files of split data written to derived_data directory
* `get_data`
    * Inputs: `name`: a string from {"train_x", "train_y", "test_x", "test_y"}
    * Outputs: A dataframe for train or test x, a factor of level 0 or 1 for train or test y
* `confusion_mat`
    * Inputs: `pred`, `true`, `title`, `file.out`
    * Outputs: ggplot object
* `phishing_pca`
    * Inputs: none
    * Outputs: ggplot object
* `run_rf`
    * Inputs: `k`, `cv`, `mtry`, `min.node.size`, `n_tree`, `seed`, `trimmed_var`, `verbose`, `file`
    * Outputs: train object from caret for the optimal model
* `SGD_logistic`
    * Inputs: `y`, `X`, `batch_size`, `learning_rate`, `epsilon`, `max_iter`
    * Outputs: List of coefficients and number of iterations
* `run_svm`
    * Inputs: **FILL IN** **************************
    * Outputs: **FILL IN** **************************
    
    

    



# Methods
## Likelihood Approach
### Generalized Linear Model

Suppose our training set has $n$ datapoints. Then: 

\noindent Let $Y$ be the $n \times 1$ response vector:
$$ Y_i = 
\begin{cases} 
1 & \text{if webpage is phishing} \\
0 & \text{if not} 
\end{cases} \quad i = 1, \ldots, n$$. 

\noindent Let $X^T$ be the $n \times (p + 1)$ design matrix: 
\[ X^T = 
\begin{bmatrix}
1 & x_{11} & \cdots & x_{1p} \\
1 & x_{21} & \cdots & x_{2p} \\
\vdots & \ddots & \vdots \\
1 & x_{n1} & \cdots & x_{np} \\
\end{bmatrix}
\]

\noindent  Let $p_i = P(Y_i=1|\mathbf{x_i^T)}$. Since our response variable is binary, we fit a logistic  model on the dataset. We use the logit link. 

$$logit(p_i) = log(\frac{p_i}{1-p_i}) =  \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} = \mathbf{x_i^T} \beta$$

\noindent Since this is binary data, the individual data come from a Bernoulli distribution. Our joint likelihood function is: 

$$L(\beta|Y, n) = \prod_{i=1}^n p_i^{I(y_i=1)} (1-p_i)^{I(y_i=0)}$$

\noindent The log-likelihood is:

\begin{align}
l(\beta|Y, n) &= \sum_{i=1}^n I(y_i=1) log(p_i) + I(y_i=0) log(1-p_i) \\
&= \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)
\end{align}

### Logistic Regression with Mini-Batch SGD

We chose to use mini-batch stochastic gradient descent to find the solution for \( \beta \). This decision was based on the principle that maximizing the log-likelihood function is analogous to minimizing its negative counterpart. Therefore, we define our cost function as \( J(\beta) = -l(\beta \mid Y, n) \). The algorithm proceeds as follows:

1. Initialize the estimate for $\beta$.
2. At each iteration, select a mini-batch of size $\alpha$ from $X^T$, denoted $\{\mathbf{x_i^T}\}$, where $i$ indexes the randomly selected rows.
3. Calculate the gradient of the negative log-likelihood, $\nabla J(\beta)$, using the mini-batch sample.
4. Update $\beta$ according to the rule: $\beta_{\text{new}} = \beta_{\text{old}} - \eta \nabla J(\beta)$, where $\eta$ is the learning rate.
5. Repeat steps 2-4 until convergence, defined as $|\beta_{\text{new}} - \beta_{\text{old}}| < \epsilon$, where $\epsilon$ is a pre-specified threshold.


\noindent The gradient of the log-likelihood for our model is detailed below (for full derivation, please see the $\textbf{Appendix B}$):

$$\nabla J(\beta)^T = [ \frac{\partial J(\beta)}{\partial \beta_0} \ldots \frac{\partial J(\beta)}{\partial \beta_p} ] = \mathbf{X^T}(\mathbf{p}-\mathbf{y})$$

\begin{align}
\frac{\partial J(\beta)}{\partial \beta_j} &= \frac{\partial J(\beta)}{\partial p_i} \cdot \frac{\partial p_i}{\partial \beta_j} \\
&= \sum_{i=1}^n - \left(\frac{y_i}{p_i} + \frac{1-y_i}{1-p_i}\right) \cdot p_i(1-p_i) x_{ij} \\
&= \sum_{i=1}^n (-y_i(1-p_i) + (1-y_i)p_i) x_{ij} \\
&= \sum_{i=1}^n (p_i - y_i) x_{ij} 
\end{align}


\noindent We initialized \(\beta\) as the zero vector at the start of the algorithm and continued iterations until convergence. Convergence was defined by the condition \( \left| \beta_{\text{new}} - \beta_{\text{old}} \right| < \epsilon \), where \(\epsilon = 10^{-4}\).

\noindent First, we verified that our SGD algorithm functioned correctly through simulation. We generated 100,000 observations with 20 covariates, each drawn from a normal distribution:

$$X_i \sim \mathcal{N}(0, 1) \text{ for } i = 1, \ldots, 20$$

Given that our dataset contains only non-negative covariate values, we transformed all covariates by taking their absolute value:

$$X_{i, \text{new}} = \left| X_i \right|$$

We then assigned the coefficients a value of 0.5 or -0.5, and the outcomes were calculated using a logistic regression model with a logit link function:

$$\text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} = \mathbf{x_i}^T \beta$$

where $\beta_0 = -0.5$, $\beta_j = 0.5$ for $j = 1, \ldots, 10$, and $\beta_j = -0.5$ for $j = 11, \ldots, 20$.

The mini-batch size ($\alpha$) was set to 128, and the learning rate ($\eta$) was set to 0.001. The algorithm converged after 962 iterations, and we calculated the mean square error (MSE) as the squared differences between the parameter estimates derived from our algorithm and those generated by the General Linear Model (GLM) function from the \textbf{stats} package in R. The MSE is $8 \times 10^{-6}$, indicating that our algorithm performs comparably to the GLM.

```{r, eval=FALSE}
# Conduct simulation
set.seed(123)
n=1e5
X.sim = abs(matrix(rnorm(n * 20), nrow = n, ncol = 20))
data.sim = as.data.frame(X.sim)
names(data.sim) = paste("X", 1:20, sep="")
beta.sim = c(-0.5,rep(0.5,10),rep(-0.5,10))
X.intercept.sim = cbind(1,X.sim)
prob = exp(X.intercept.sim %*% beta.sim) / (1+ exp(X.intercept.sim %*% beta.sim))
data.sim$y.sim = rbinom(n,1,prob)
mean(data.sim$y.sim)
fit.sim = glm(data = data.sim, y.sim ~.,family = "binomial")
coef(fit.sim)
lg.sim = LogisticRegression(data.sim$y.sim, X.sim, epsilon = 1e-4, batch_size = 128, learning_rate=1e-3, max_iter = 50000)
cat("MSE with true beta: ",mean((lg.sim$coefficients - beta.sim)^2))
cat("MSE with GLM estimates: ",mean((lg.sim$coefficients - coef(fit.sim))^2))
```

\noindent Next, we applied the SGD algorithm to the phishing dataset, carefully tuning the mini-batch size ($\eta$) and learning rate ($\alpha$) to optimize performance. We removed the covariates that had more than 99\% missing data: $\textit{n_exclamation}, \textit{n_space}, \textit{n_tilde}, \textit{n_comma}, \textit{n_plus}, \textit{n_asterisk}, \textit{n_hashtag}, \textit{n_dollar}$. These variables were so sparse that we believed including them in the model would not be beneficial. We selected the optimal hyperparameters ($\alpha, \eta$) by evaluating the convergence speed, and Kappa. Overall, we chose a mini-batch size ($\alpha$) of 256 and a learning rate ($\eta$) of 0.01 for our final model, since this model has the best Kappa as well as good convergence speed. The accuracy and F1 score of our final model is 0.859 and 0.893, respectively.

```{r, eval=FALSE}
### tune parameters, NO NEED to run when compiling report because of time-consuming ###

# read training and testing set
train_x = get_data('train_x')
train_y = get_data('train_y')
test_x = get_data('test_x')
test_y = get_data('test_y')
y.train = train_y %>% unlist() %>% as.vector()
X.train = train_x %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()
y.test = test_y %>% unlist() %>% as.vector()
X.test = test_x %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()

tune = as.data.frame(matrix(c(rep(NA,12*5)),ncol=5))
colnames(tune) = c("batch_size","learning_rate","iteration","Accuracy","F1_score")


set.seed(123) # for shuffle in mini batch
X.intercept.test = cbind(1,X.test)
i=1
for (b in c(32,64,128,256)){
    for (lr in c(1e-2, 1e-3, 1e-4)){
    print(i)
    tune$batch_size[i] = b
    tune$learning_rate[i] = lr
    lgres = SGD_logistic(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
    tune$iteration[i] = lgres$iter
    coef = lgres$coefficients
    prob = exp(X.intercept.test %*% coef) / (1+ exp(X.intercept.test %*% coef))
    y.pred = ifelse(prob >= 0.5,1,0)
    accuracy = sum(1*(y.test == y.pred)) / length(y.test)
    cat("Accuracy:",accuracy)
    tune$Accuracy[i] = accuracy
    conf_matrix = confusionMatrix(as.factor(y.pred), as.factor(y.test))
    f1_score = conf_matrix$byClass['F1']
    cat("F1 score:",f1_score)
    tune$F1_score[i] = f1_score
    kappa = kappa2(data.frame(observed = y.test, predicted = y.pred))$value
    cat("Kappa:", kappa)
    tune$Kappa[i] = kappa
    i=i+1
    }
}
```

```{r}
# create a data frame for showing the tuning results
compare = data.frame(
  batch_size = c(32, 32, 32, 64, 64, 64, 128, 128, 128, 256, 256, 256),
  learning_rate = c(0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001, 0.01, 0.001, 0.0001),
  iteration = c(14582, 9643, 11364, 15856, 10476, 10829, 9864, 10795, 10415, 9513, 11118, 11120),
  Accuracy = c(0.8500125, 0.8534599, 0.8565576, 0.8464152, 0.8577067, 0.8561079, 0.8584562, 0.8583562, 0.8578066, 0.8593555, 0.8568574, 0.8564577),
  F1_Score = c(0.8900608, 0.8918949, 0.8931721, 0.8887924, 0.8937393, 0.8930640, 0.8924164, 0.8928855, 0.8936870, 0.8926023, 0.8932842, 0.8930738),
  Kappa = c(0.6571370, 0.6668623, 0.6765103, 0.6451411, 0.6798314, 0.6749178, 0.6861028, 0.6845838, 0.6803748, 0.6892609, 0.6774683, 0.6763456)
)

# Create the table
comparison <- kable(compare, format = "html", table.attr = "style='width:100%;'") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# Print
comparison
```

\noindent We obtained the estimated coefficients from the training set and used them to predict the binary outcome on the test set. The predictive accuracy of our gradient descent algorithm is approximately 0.858 and F1 score is approximately 0.894.

```{r,eval=FALSE}
## NO NEED to run in knitting to final report##

# read training and testing set
y.train = fread("derived_data/train_y.csv") %>% unlist() %>% as.vector()
X.train = fread("derived_data/train_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()
y.test = fread("derived_data/test_y.csv") %>% unlist() %>% as.vector()
X.test = fread("derived_data/test_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()

# refit the best model on training
b=64
lr=0.001
lgres.train = SGD_logistic(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
coef.train = lgres.train$coefficients

# predict outcomes on testing
X.intercept.test = cbind(1,X.test)
prob = exp(X.intercept.test %*% coef.train) / (1+ exp(X.intercept.test %*% coef.train))
y.pred = ifelse(prob >= 0.5,1,0)
accuracy = sum(1*(y.test == y.pred)) / length(y.test)
cat("Accuracy:",acurracy)

library(caret)
conf_matrix = confusionMatrix(as.factor(y.pred), as.factor(y.test))
f1_score = conf_matrix$byClass['F1']
cat("F1 score:",f1_score)
```

## Machine Learning approaches

## Random Forest

We utilized Random Forest implemented in R Package Ranger to construct our classification
model, same as the GLM approach, the training
set with 80% samples were used for hyperparameter tuning as well as final model training,
which was then applied on the held-out test set for performance evaluation. 

### Full Model

For the full model, all the 19 features were included for classification. For hyperparameter
tunning, We used Caret to conduct 5-fold Cross Validation For tuning parameter
grids, We considered the following combinations:

```{r}
parameters <- c("mtry", "min.node.size", "splitrule", "num.trees")
ranges <- c("c(2,10,19)", "c(1,5)", "c('gini', 'extratree')", "c(5,50,500)")

definitions <- c(
  "Number of variables sampled as candidates",
  "Minimal node size to split at",
  "Split to minimize gini decrease vs splits nodes by choosing cut-points fully at random",
  "Number of trees to be ensembled"
)

param_df <- data.frame(
  parameter = parameters, 
  range = ranges, 
  definition = definitions
)

kable(param_df, format = "html", table.attr = "style='width:100%;'") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

We used Kappa to select the best model. From the following cross validation result, we decided that the optimal parameter combination is `mtry = 10, min.node.size = 5, splitrule='gini',num.trees=50`.

```{r}
plot_rf_cv(rf_full_cv,title='Full Model CV result')
```
Using the selected parameters, we fitted the final model on the complete training set and tested its prediction accuracy on the test set, with the confusion matrix as well as performance metrics listed below. The final model achieved Kappa of 0.7669 and F1 score of 0.9163.

```{r,message=F}
train_x = get_data('train_x')
train_y = get_data('train_y')
test_x = get_data('test_x')
test_y = get_data('test_y')
start = Sys.time()
full <- run_rf(cv=F,mtry=10,splitrule='gini',n_tree=50,min.node.size=5,verbose=T)
(paste('Training time:',as.numeric(Sys.time()-start)))
confusion_mat(predict(full,test_x),title="Full model prediction confusion matrix",file.out = "figures/full_model_confusion_mat.jpg")

```

We then utilized the nature of random forest's capability of variable selection to 
investigate the relationship between the feature importance (by mean decrease in Gini) and 
its sparsity. For dense variables, variable importance is not directly correlated to percentage
of 0, which with why we would need variable selection methods to decide the importance variable
subset; for sparse variables with over 50% of 0, there seems to be decrease in importance 
as the sparsity increases, which is reasonable due to reduced amount of information.

```{r,warning=F}
library(ranger)
perc_0 <-
    train_x %>% summarise(across(everything(), ~ mean(.x == 0)))
df <-
    data.frame(importance = importance(full$finalModel),
               percent_of_zero = t(perc_0)[, 1])

kable(df, format = "html", table.attr = "style='width:100%;'") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

ggplot(df, aes(x = percent_of_zero, y = importance)) +
    theme_bw() +
    geom_point(color = 'blue', size = 2) +
    geom_line(size = 1) +
    labs(x = 'Percentage of 0', y = 'Gini importance') +
    geom_vline(xintercept = 0.99, linetype = 2)
```

### Reduced Model

Although random forest model does not suffer from burden of convergence with sparse features,
we still attempted to fit a reduced model by removing the 8 features with the percentage of 0
greater than 99%. We re-tuned the parameter based on training set the subset of features via 
5-fold cross-validation, selecting out the best parameter combination as `mtry = 6, min.node.size = 1, splitrule='gini',num.trees=500`.

```{r}
plot_rf_cv(rf_reduce_cv,title='Reduced Model CV result')
```

We fitted the final reduced model with these parameters on the complete training set and tested its prediction accuracy on the test set and listed the confusion matrix as well as performance metrics. The reduced model achieved Kappa of 0.7642 and F1 score of 0.9150. On one hand, there is barely any impact on the prediction performance after removing the 8 extremely sparse features, consistent with their low importance in the full model; on the other hand, training the reduced model takes longer time compared to the full model (since the optimal number of trees were selected as 500 compared to 50). Because of this, we retained both models for the final comparison.


```{r,message=F}
train_x = get_data('train_x')
train_y = get_data('train_y')
test_x = get_data('test_x')
test_y = get_data('test_y')
start = Sys.time()
reduce <- run_rf(cv=F,mtry=6,splitrule='gini',n_tree=500,min.node.size=1,verbose=T,
                     trimmed_var=c('n_exclamation',
                                   'n_plus',
                                   'n_tilde',
                                   'n_space',
                                   'n_comma',
                                   'n_asterisk',
                                   'n_dollar',
                                   'n_hashtag'))
paste('Training time:',as.numeric(Sys.time()-start))
confusion_mat(predict(reduce,test_x),title="Reduced model prediction confusion matrix",file.out = "figures/reduced_model_confusion_mat.jpg")

```

## Support Vector Machines

### Full Model

We trained the model using 5-fold cross-validation on training set using all 19 features. 
We compared Linear SVM with Radial basis function (RBF) SVM. For the tuning parameter in Linear SVM, we trained the model using C=seq(0, 2, length = 15), where C (cost) controls the trade-off between achieving a low error on the training data and maintaining model simplicity for better generalization. For the tuning parameters in RBF SVM, we trained the model using the combination of C=seq(0, 2, length = 10) and Sigma = c(0.001, 0.1,1,10,100), where sigma controls the model's level of non-linearity. A larger sigma value results in a smoother decision boundary.
After tuning, we chose paramter with highest kappa. For the Linear SVM the optimal C is 1.43 and for the RBF SVM, the optimal C is 2 and sigma is 1.
 


```{r,eval=F}
library(caret)
train_x = get_data('train_x')
train_y = get_data('train_y')
test_x = get_data('test_x')
test_y = get_data('test_y')
#Run Full model Linear SVM, it takes a while to run
run_svm(seed = 111,reduced = F,RBF = F,train_x =train_x,test_x = test_x,train_y =train_y,test_y = test_y  )
#Run Full model RBF SVM, do not run, take a long time
run_svm(seed = 111,reduced = F,RBF = T,train_x =train_x,test_x = test_x,train_y =train_y,test_y = test_y  )
```

```{r, echo=FALSE, fig.align="center"}
###variable importance
knitr::include_graphics("figures/SVM_impt.png")
```

```{r, echo=FALSE, fig.align="center"}
###Linear svm confusion matrix
knitr::include_graphics("figures/linear_svm.png")
```

```{r, echo=FALSE, fig.align="center"}
###Linear svm confusion matrix
knitr::include_graphics("figures/RBF_svm.png")
```

In the SVM model, the most 2 important variables were "url_length" and "n_slash". And the least important variables include "n_space", "n_asterisk", "n_dollar","n_plus", "n_hashtag", which were similar as in the random forest model. The least important variables had 0 percentage bigger than 99%. 

The best performed linear SVM had F1 score 0.894 and Kappa 0.681 on the test. And the best performed RBF SVM had F1 score 0.911 and Kappa 0.747 on the test. The RBF SVM performed better than Linear SVM.

### Reduced model

For reduced model,variables with 0 percentage bigger than 99% were excluded for the model fitting. 11 variables left after removing "n_exclamation", "n_plus", "n_tilde", "n_space", "n_comma", "n_asterisk", "n_dollar", "n_hashtag".


```{r,eval=F}
#Run Reduced model Linear SVM, it takes a while to run
run_svm(seed = 111,reduced = T,RBF = F,train_x =train_x,test_x = test_x,train_y =train_y,test_y = test_y  )
#Run Reduced model RBF SVM, do not run, take a long time
run_svm(seed = 111,reduced = T,RBF = T,train_x =train_x,test_x = test_x,train_y =train_y,test_y = test_y  )
```

Following the same tuning procedure, in the reduced model setting, the optimal C for Linear SVM was 1.71 and the optimal C and Sigma for 1.2 and 1.1.

The best performed linear SVM had F1 score 0.894 and Kappa 0.68 on the test. And the best performed RBF SVM had F1 score 0.910 and Kappa 0.745 on the test. The reduced model did not perform better or worse than the full model. 

# Discussion

## Final Model Comparison

In the table below, we compare the performance of seven tuned models on classification of the test set observations. These models are:

* Logistic Regression with Mini-Batch Stochastic Gradient Descent
* Random Forest (Full Model)
* Random Forest (Reduced Model)
* Support Vector Machine with Linear Kernel (Full Model)
* Support Vector Machine with Radial Basis Function Kernel (Full Model)
* Support Vector Machine with Linear Kernel (Reduced Model)
* Support Vector Machine with Radial Basis Function Kernel (Reduced Model)

Although we calculated both Cohen's Kappa and F1 Score for each model (see table below), we tuned each model based on best Cohen's Kappa value. Thus, we use the best Kappa metric to compare the final models' performances on the test set. Note that we calculated F1 Score for each model just to get a sense of accuracy for each, but all tuning and model comparison is based on best Kappa.

```{r, echo=FALSE, fig.cap="Figure 2.", fig.align="center"}
knitr::include_graphics("figures/modelcomparison.png")
```

In terms of best Kappa on the test set, these models perform fairly similarly, with the random forest full model very slightly outperforming the random forest reduced model and the SVMs with radial basis function kernel. In terms of training time, the random forest models trained with the `ranger` package in R were clearly the fastest. The SVM models fit with `caret` took extremely long to train for both the full and reduced models, so even though the SVM models with RBF kernel (both full and reduced) performed decently well, we would still not prefer to use them in the future. The GLM we fit from scratch without a built-in R function did not take nearly as long as SVM but was still a bit slower than the random forest models. However, it is still comparable in terms of prediction error on the test set, which is good to see.

In choosing a final "best" model, we consider model complexity (Occam's Razor), training time, and prediction performance on the test set by best Kappa. Based on all of these factors, we would choose the Random Forest Reduced Model as the best model performing model in this analysis.

## Limitations and Future Work

Although this project was fairly straightforward in terms of data cleanliness and modeling the classification problem, there were several limitation that we encountered which can motivate future work on this project. First, the data set is anonymized which means the raw URLs are not provided; only the character count features of each URL are given. This limits the analysis, visualizations, and feature engineering potential. If we had the raw URLs for each website, then we could definitely consider applying deep learning methods to classify the URLs themselves, rather than just use the character count features. In addition, we could feature engineer our own features of interest that were not contained in the original data, such as ratio of alphanumeric characters to symbols or capital letters, and we would not necessarily need to be restricted to sparse zero-inflated Poisson counts as covariates. This sparsity in the original data also created difficulty with convergence of mini-batch stochastic gradient for the GLM, so the sparsest covariates (>99% sparse) had to be pre-filtered from the model. Finally, we could consider trying more complex GLMs to compare to the machine learning methods, such as GLMs with interaction terms or different link functions. 


# References

GLM: 

1. https://web.stanford.edu/~jurafsky/slp3/5.pdf 

2. https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm 

3. https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/ 

4. https://rpubs.com/enwuliu/1011024 

RF:

https://doi.org/10.1007/s10994-006-6226-1

# Apendices

## Appendix A

\noindent Expressing the logit function \( logit(p_i) \) in terms of \( p_i \):
\[logit(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \mathbf{x_i^T}\beta\]

\noindent This equation can be rewritten to solve for \( p_i \):
\[\frac{p_i}{1-p_i} = \exp(\mathbf{x_i^T}\beta)\]

\noindent Multiplying both sides by \( 1-p_i \) gives:
\[p_i = (1-p_i) \exp(\mathbf{x_i^T}\beta)\]

\noindent Expanding and rearranging terms to isolate \( p_i \) on one side:
\begin{align*}
p_i &= \exp(\mathbf{x_i^T}\beta) - p_i \exp(\mathbf{x_i^T}\beta) \\
p_i + p_i \exp(\mathbf{x_i^T}\beta) &= \exp(\mathbf{x_i^T}\beta) \\
p_i(1 + \exp(\mathbf{x_i^T}\beta)) &= \exp(\mathbf{x_i^T}\beta)
\end{align*}

\noindent Solving for \( p_i \):
\[p_i = \frac{\exp(\mathbf{x_i^T}\beta)}{1 + \exp(\mathbf{x_i^T}\beta)}\]

\noindent By factoring out \( \exp(\mathbf{x_i^T}\beta) \) in the denominator, the expression simplifies further:
\[p_i = \frac{1}{1 + \exp(-\mathbf{x_i^T}\beta)}\]

## Appendix B

\noindent Here are the calculations for the partial derivatives required for the gradient function:

\begin{align}
\frac{\partial J(\beta)}{\partial p_i} 
&= \frac{\partial}{\partial p_i} - \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)\\
&= \sum_{i=1}^n - \frac{y_i}{p_i} + \frac{(1-y_i)}{1-p_i}
\end{align}

\begin{align}
\frac{\partial p_i}{\partial \beta_j} &= \frac{\partial p_i}{\partial \beta_j}  (1 + \exp(-\mathbf{x_i^T} \beta))^{-1} \\
&= exp(-\mathbf{x_i^T} \beta) (1+ exp(-\mathbf{x_i^T}\beta))^{-2} x_{ij} \\ 
&=  \frac{(1+exp(-\mathbf{x_i^T}\beta))-1}{(1+ exp(-\mathbf{x_i^T}\beta)) (1+ exp(-\mathbf{x_i^T}\beta))} x_{ij} \\ 
&=  \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))} (1 - \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))}) x_{ij} \\
&=  p_i (1-p_i) x_{ij}
\end{align}

