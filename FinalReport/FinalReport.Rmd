---
title: "BIOS 735 Final Project"
author: "Madhuri Raman, Will Tang, Xuejun Sun, Sophie Shan, Jiawen Du"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: yes
    keep_tex: yes 
subtitle: Group 1
---

# Introduction

In the digital age, phishing is a common practice in which cybercriminals attempt to steal sensitive, personal information from users, such as their usernames/passwords, social-security number, credit card information, etc. Phishing can occur in many different forms, but one common way is through websites. You have probably received suspicious emails before from unknown senders. These emails usually contain phrases to bait your interest and may contain a link to a website that sounds interesting, but really is just collecting your personal information. We want to avoid clicking on phishing links, while also not missing out on important non-phishing links that we need to access. 

## Motivation

The goal of this project is to be able to predict whether or not a website link is a phishing website or not, based on only features of the URL. If we ever receive an email with a link from an unknown sender, but we aren't sure if it is suspicious, we hope that this data analysis can: 
1. Predict whether the URL is phishing or not, and
2. Provide insight into what features of the URL are most important to look out for when judging whether the link is phishing or not.


## Data

In order to achieve these goals, we work with a data set from Kaggle (linked [here](https://www.kaggle.com/datasets/danielfernandon/web-page-phishing-dataset)) which contains 100,007 website URLs, 19 features of these URLs, and an indicator (0 or 1) of whether the URL is a phishing website or not. The 19 features are characteristics of each URL and includes: 

* url length
* number of dots in the URL
* number of hyphens in the URL
* number of underlines in the URL
* number of "+" characters in the URL
* number of redirections (number of times the URL redirects you to another website)

etc. Below we conduct exploratory data analysis to examine the univariate and bivariate distributions of the features and response variable (indicator of phishing).

### EDA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Sparsity

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Missingness?

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi iyean blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

#### Visualize distributions, correlations, outliers

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.

##### Histograms, corrplots, PCA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.


#### Preview of Final Data

```{r}
library()

```

## Data Splitting and Setup

\noindent We first split out dataset into a training (80\%) and test set (20\%). Suppose our training set has $n$ datapoints. Then: 

\noindent Let $Y$ be the $n \times 1$ response vector:
$$ Y_i = 
\begin{cases} 
1 & \text{if webpage is phishing} \\
0 & \text{if not} 
\end{cases} \quad i = 1, \ldots, n$$. 

\noindent Let $X^T$ be the $n \times (p + 1)$ design matrix: 
\[ X^T = 
\begin{bmatrix}
1 & x_{11} & \cdots & x_{1p} \\
1 & x_{21} & \cdots & x_{2p} \\
\vdots & \ddots & \vdots \\
1 & x_{n1} & \cdots & x_{np} \\
\end{bmatrix}
\]

We use the above framework for the following data analysis, both for the likelihood-based method and machine learning methods.

# Packaging

Insert info about package and functions

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sed tortor mauris. Nulla auctor a mi in blandit. Nulla tellus metus, iaculis in ullamcorper et, vulputate ac quam. Aenean nec est lectus. Nullam cursus at lacus ut elementum. Quisque at eros eget nibh auctor pellentesque quis non arcu. Proin tincidunt libero vitae tellus vestibulum rhoncus. Cras blandit condimentum eleifend. Integer ut enim nec elit vulputate maximus. Donec in tellus dui. Donec vel purus sollicitudin, hendrerit augue ac, semper tortor.


# Methods
## Likelihood Approach
### Generalized Linear Model

\noindent  Let $p = P(Y=1|X)$. Since our response variable is binary, we fit a logistic  model on the dataset. We use the logit link. 

$$logit(p_i) = log(\frac{p_i}{1-p_i}) =  \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} = \mathbf{x_i^T} \beta$$

\noindent Since this is binary data, the individual data come from a Bernoulli distribution. Our joint likelihood function is: 

$$L(\beta|Y, n) = \prod_{i=1}^n p_i^{I(y_i=1)} (1-p_i)^{I(y_i=0)}$$

\noindent The log-likelihood is:

\begin{align}
l(\beta|Y, n) &= \sum_{i=1}^n I(y_i=1) log(p_i) + I(y_i=0) log(1-p_i) \\
&= \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)
\end{align}

### Logistic Regression with Mini-Batch SGD

We chose to use mini-batch stochastic gradient descent to find the solution for \( \beta \). This decision was based on the principle that maximizing the log-likelihood function is analogous to minimizing its negative counterpart. Therefore, we define our cost function as \( J(\beta) = -l(\beta \mid Y, n) \). The algorithm proceeds as follows:

1. Initialize the estimate for $\beta$.
2. At each iteration, select a mini-batch of size $\alpha$ from $X^T$, denoted $\{\mathbf{x_i^T}\}$, where $i$ indexes the randomly selected rows.
3. Calculate the gradient of the log-likelihood, $\nabla J(\beta)$, using the mini-batch sample.
4. Update $\beta$ according to the rule: $\beta_{\text{new}} = \beta_{\text{old}} - \eta \nabla J(\beta)$, where $\eta$ is the learning rate.
5. Repeat steps 2-4 until convergence, defined as $|\beta_{\text{new}} - \beta_{\text{old}}| < \epsilon$, where $\epsilon$ is a pre-specified threshold.


\noindent The gradient of the log-likelihood for our model is detailed below (for full derivation, please see the $\textbf{Appendix B}$):

$$\nabla J(\beta)^T = [ \frac{\partial J(\beta)}{\partial \beta_0} \ldots \frac{\partial J(\beta)}{\partial \beta_p} ] = \mathbf{X^T}(\mathbf{p}-\mathbf{y})$$

\begin{align}
\frac{\partial J(\beta)}{\partial \beta_j} &= \frac{\partial J(\beta)}{\partial p_i} \cdot \frac{\partial p_i}{\partial \beta_j} \\
&= \sum_{i=1}^n - \left(\frac{y_i}{p_i} + \frac{1-y_i}{1-p_i}\right) \cdot p_i(1-p_i) x_{ij} \\
&= \sum_{i=1}^n (-y_i(1-p_i) + (1-y_i)p_i) x_{ij} \\
&= \sum_{i=1}^n (p_i - y_i) x_{ij} 
\end{align}


\noindent We initialized \(\beta\) as the zero vector at the start of the algorithm and continued iterations until convergence. Convergence was defined by the condition \( \left| \beta_{\text{new}} - \beta_{\text{old}} \right| < \epsilon \), where \(\epsilon = 10^{-4}\).

\noindent First we conducted simulation for our algorithm. We generate 100k observations with 20 covariates from normal distribution. Since our our data has non-negative covariates values, we take the absolute value of all covariates. Then we generated the coefficients to be 0.5 or -0.5, and outcome is calculated by a logistic regression model with logit link. We chose a proper mini-batch size ($\alpha$) to be 128 and and the learning rate ($\eta$) to be 0.001. The algorithm converged after 962 iterations and we observed that the mean square error (MSE) between coefficients estimates by our algorithm and the coefficients estimated by GLM is only $8 * 10^{-6}$, which indicate that our algorithm can perform as well as GLM.

```{r, eval=FALSE}
library(data.table)
library(dplyr)

# Conduct simulation
set.seed(123)
n=1e5
X.sim = abs(matrix(rnorm(n * 20), nrow = n, ncol = 20))
data.sim = as.data.frame(X.sim)
names(data.sim) = paste("X", 1:20, sep="")
beta.sim = c(-0.5,rep(0.5,10),rep(-0.5,10))
X.intercept.sim = cbind(1,X.sim)
prob = exp(X.intercept.sim %*% beta.sim) / (1+ exp(X.intercept.sim %*% beta.sim))
data.sim$y.sim = rbinom(n,1,prob)
mean(data.sim$y.sim)
fit.sim = glm(data = data.sim, y.sim ~.,family = "binomial")
coef(fit.sim)
lg.sim = LogisticRegression(data.sim$y.sim, X.sim, epsilon = 1e-4, batch_size = 128, learning_rate=1e-3, max_iter = 50000)
cat("MSE with true beta: ",mean((lg.sim$coefficients - beta.sim)^2))
cat("MSE with GLM estimates: ",mean((lg.sim$coefficients - coef(fit.sim))^2))
```

\noindent Further, we apply our algorithm on the phishing dataset. First we split our phishing dataset to training (80%) and testing (20%). Then we optimized both the mini-batch size ($\alpha$) and the learning rate ($\eta$), selecting the best hyperparameters based on convergence speed and Mean Squared Error (MSE) on training set. Here, MSE was calculated as the squared differences between the parameter estimates obtained by our method and those provided by the General Linear Model (GLM) function in the \textbf{stats} package in R. Based on MSE, we chose batch size = 128 and learning rate = 0.01 for our final model. 

| batch size | learning rate | iteration | MSE       |
|------------|---------------|-----------|-----------|
| 32         | 0.01          | 14582     | 3.05294096|
| 32         | 0.001         | 9643      | 0.19290544|
| 32         | 0.0001        | 11364     | 1.12656500|
| 64         | 0.01          | 15856     | 0.64438685|
| 64         | 0.001         | 10476     | 0.36867774|
| 64         | 0.0001        | 10829     | 1.54867577|
| 128        | 0.01          | 9864      | 0.02094163|
| 128        | 0.001         | 10795     | 0.65899539|
| 128        | 0.0001        | 10415     | 1.91463586|
| 256        | 0.01          | 9513      | 0.12832514|
| 256        | 0.001         | 11118     | 1.00933585|
| 256        | 0.0001        | 11120     | 2.16481173|

```{r, eval=FALSE}
# split into training and testing
y.train = fread("derived_data/train_y.csv") %>% unlist() %>% as.vector()
X.train = fread("derived_data/train_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()
y.test = fread("derived_data/test_y.csv") %>% unlist() %>% as.vector()
X.test = fread("derived_data/test_x.csv") %>% select(-n_exclamation, -n_space, -n_tilde, -n_comma, -n_plus, -n_asterisk, -n_hastag, -n_dollar) %>% as.matrix()

### tune parameters ###
# fit GLM
fit = glm(y.train ~ X.train, family="binomial")
coef = coef(fit)

# tune our hyperparameters
set.seed(123)

tune = as.data.frame(matrix(c(rep(NA,12*4)),ncol=4))
colnames(tune) = c("batch_size","learning_rate","iteration","MSE")

i=1
set.seed(123) # for shuffle in mini batch
for (b in c(32,64,128,256)){
    for (lr in c(1e-2, 1e-3, 1e-4)){
    print(i)
    tune$batch_size[i] = b
    tune$learning_rate[i] = lr
    lgres = LogisticRegression(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
    tune$iteration[i] <- lgres$iter
    print(lgres$coefficients)
    print(coef)
    tune$MSE[i] <- mean((coef(fit) - lgres$coefficients)^2)
    print(mean((coef - lgres$coefficients)^2))
    print(tune[i,])
    i=i+1
    }
}
# print(tune)
```

\noindent We obtained the coefficients estimated from training set and predict the binary outcome on testing. The accuracy of our gradient descend algorithm is around 0.851. 


```{r,eval=FALSE}
b=128
lr=0.01
lgres.train = SGD_logistic(y.train,X.train, batch_size = b, learning_rate = lr, epsilon = 1e-4, max_iter = 50000)
coef.train = lgres.train$coefficients
X.intercept.test = cbind(1,X.test)
prob = exp(X.intercept.test %*% coef.train) / (1+ exp(X.intercept.test %*% coef.train))
y.pred = ifelse(prob >= 0.5,1,0)
acurracy = sum(1*(y.test == y.pred)) / length(y.test)
print(acurracy)
```

## Machine Learning approaches

### Random Forest

### Support Vector Machines
```{r}



```

# Final predictions on test set (to "choose" best model)

```{r}



```

# Conclusion

## General thoughts

## Limitations of this project

## Future work

# Apendices

## Appendix A

\noindent Expressing the logit function \( logit(p_i) \) in terms of \( p_i \):
\[logit(p_i) = \log\left(\frac{p_i}{1-p_i}\right) = \mathbf{x_i^T}\beta\]

\noindent This equation can be rewritten to solve for \( p_i \):
\[\frac{p_i}{1-p_i} = \exp(\mathbf{x_i^T}\beta)\]

\noindent Multiplying both sides by \( 1-p_i \) gives:
\[p_i = (1-p_i) \exp(\mathbf{x_i^T}\beta)\]

\noindent Expanding and rearranging terms to isolate \( p_i \) on one side:
\begin{align*}
p_i &= \exp(\mathbf{x_i^T}\beta) - p_i \exp(\mathbf{x_i^T}\beta) \\
p_i + p_i \exp(\mathbf{x_i^T}\beta) &= \exp(\mathbf{x_i^T}\beta) \\
p_i(1 + \exp(\mathbf{x_i^T}\beta)) &= \exp(\mathbf{x_i^T}\beta)
\end{align*}

\noindent Solving for \( p_i \):
\[p_i = \frac{\exp(\mathbf{x_i^T}\beta)}{1 + \exp(\mathbf{x_i^T}\beta)}\]

\noindent By factoring out \( \exp(\mathbf{x_i^T}\beta) \) in the denominator, the expression simplifies further:
\[p_i = \frac{1}{1 + \exp(-\mathbf{x_i^T}\beta)}\]

## Appendix B

\noindent Here are the calculations for the partial derivatives required for the gradient function:

\begin{align}
\frac{\partial J(\beta)}{\partial p_i} 
&= \frac{\partial}{\partial p_i} - \sum_{i=1}^n y_i log(p_i) + (1-y_i) log(1-p_i)\\
&= \sum_{i=1}^n - \frac{y_i}{p_i} + \frac{(1-y_i)}{1-p_i}
\end{align}

\begin{align}
\frac{\partial p_i}{\partial \beta_j} &= \frac{\partial p_i}{\partial \beta_j}  (1 + \exp(-\mathbf{x_i^T} \beta))^{-1} \\
&= exp(-\mathbf{x_i^T} \beta) (1+ exp(-\mathbf{x_i^T}\beta))^{-2} x_{ij} \\ 
&=  \frac{(1+exp(-\mathbf{x_i^T}\beta))-1}{(1+ exp(-\mathbf{x_i^T}\beta)) (1+ exp(-\mathbf{x_i^T}\beta))} x_{ij} \\ 
&=  \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))} (1 - \frac{1}{(1+ exp(-\mathbf{x_i^T}\beta))}) x_{ij} \\
&=  p_i (1-p_i) x_{ij}
\end{align}

